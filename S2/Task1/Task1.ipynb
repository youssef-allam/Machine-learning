{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8AtaparHdlgB",
        "gmpNte01XY8G"
      ],
      "authorship_tag": "ABX9TyPfdz2BT0Es2gYS55P+O2KA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssef-allam/Machine-learning/blob/main/S2/Task1/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Task1**"
      ],
      "metadata": {
        "id": "2IyOi2tJPQeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **import libraries**"
      ],
      "metadata": {
        "id": "jSIB8Zv3PYzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWhjJgU8LnHq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D , Activation , Dropout\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Xc_AEYvPO0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load data (Cifar100)**"
      ],
      "metadata": {
        "id": "Fox7bG_FPvX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data(label_mode=\"fine\")"
      ],
      "metadata": {
        "id": "GgZDVs53P6aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### add lables then split data"
      ],
      "metadata": {
        "id": "dknwnfGLQJgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.append(X_train.reshape((50000,-1)), y_train, axis=1)#adding train labels\n",
        "train, val = train_test_split(data, test_size = 0.2,random_state=43)#spliting train data\n",
        "\n",
        "test = np.append(X_test.reshape((10000,-1)), y_test, axis=1)#adding test labels\n",
        "\n",
        "inputShape= [train[0,:-1].shape[0]] # input shape"
      ],
      "metadata": {
        "id": "TYkDkD3gQNU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "L52Cc6z7SN2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "#build\n",
        "#preprocessing to aviod errors of convergence\n",
        "pipe = make_pipeline(StandardScaler(), LogisticRegression()) \n",
        "pipe.fit(train[:,:-1],train[:,-1])  \n",
        "\n",
        "#validate\n",
        "y_pred = pipe.predict(val[:,:-1])\n",
        "print(\"validation accuarcy:\",accuracy_score(val[:,-1], y_pred))\n",
        "\n",
        "#test\n",
        "y_pred = pipe.predict(test[:,:-1])\n",
        "print(\"validation accuarcy:\",accuracy_score(test[:,-1], y_pred))\n"
      ],
      "metadata": {
        "id": "D2cEOGUaSV2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018d5f3e-5350-4467-d456-ea360812f865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation accuarcy: 0.1477\n",
            "validation accuarcy: 0.1455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boWM9X8Mn9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model"
      ],
      "metadata": {
        "id": "8AtaparHdlgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classes_num = 100\n",
        "\n",
        "model = Sequential()#building Sequential model\n",
        "\n",
        "# Parse numbers as floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize data\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "\n",
        "model.add(layers.InputLayer(input_shape = inputShape))# input shape\n",
        "model.add(layers.Dense(1000, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal(), name=\"layer1\"))\n",
        "model.add(layers.Dense(800, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal(), name=\"layer2\"))\n",
        "model.add(layers.Dense(650, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal(), name=\"layer3\"))\n",
        "model.add(layers.Dense(330, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal(), name=\"layer4\"))\n",
        "model.add(layers.Dense(250, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal(), name=\"layer5\"))\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(classes_num, activation=\"softmax\" , kernel_initializer=tf.keras.initializers.HeNormal(), name=\"output_layer\"))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    #selecting adam optimizer\n",
        "    optimizer=keras.optimizers.Adam(.001),  \n",
        "\n",
        "    #loss function\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Fit data to model\n",
        "history = model.fit(\n",
        "    data[:,:-1],data[:,-1],                   \n",
        "    epochs=32,  \n",
        "    batch_size=100,\n",
        "    validation_split=.2)\n",
        "\n",
        "# the result\n",
        "\n",
        "results = model.evaluate(test[:,:-1], test[:,-1], batch_size=100)\n",
        "print(f'loss: {results[0]} , accuarcy: {results[1]} ' )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLOaB_VkhuLu",
        "outputId": "245290ac-43fb-418a-ca48-e1a0ed4587df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "400/400 [==============================] - 19s 45ms/step - loss: 59.5898 - sparse_categorical_accuracy: 0.0125 - val_loss: 4.5987 - val_sparse_categorical_accuracy: 0.0133\n",
            "Epoch 2/32\n",
            "400/400 [==============================] - 20s 50ms/step - loss: 4.5754 - sparse_categorical_accuracy: 0.0174 - val_loss: 4.5443 - val_sparse_categorical_accuracy: 0.0221\n",
            "Epoch 3/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 4.5261 - sparse_categorical_accuracy: 0.0228 - val_loss: 4.5102 - val_sparse_categorical_accuracy: 0.0215\n",
            "Epoch 4/32\n",
            "400/400 [==============================] - 17s 44ms/step - loss: 4.4527 - sparse_categorical_accuracy: 0.0277 - val_loss: 4.3962 - val_sparse_categorical_accuracy: 0.0363\n",
            "Epoch 5/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 4.3662 - sparse_categorical_accuracy: 0.0358 - val_loss: 4.3549 - val_sparse_categorical_accuracy: 0.0376\n",
            "Epoch 6/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 4.2771 - sparse_categorical_accuracy: 0.0444 - val_loss: 4.2613 - val_sparse_categorical_accuracy: 0.0429\n",
            "Epoch 7/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 4.2159 - sparse_categorical_accuracy: 0.0516 - val_loss: 4.2078 - val_sparse_categorical_accuracy: 0.0545\n",
            "Epoch 8/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 4.1561 - sparse_categorical_accuracy: 0.0617 - val_loss: 4.1263 - val_sparse_categorical_accuracy: 0.0707\n",
            "Epoch 9/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 4.0835 - sparse_categorical_accuracy: 0.0734 - val_loss: 4.1093 - val_sparse_categorical_accuracy: 0.0700\n",
            "Epoch 10/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 4.0192 - sparse_categorical_accuracy: 0.0825 - val_loss: 4.0959 - val_sparse_categorical_accuracy: 0.0821\n",
            "Epoch 11/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.9625 - sparse_categorical_accuracy: 0.0904 - val_loss: 3.9642 - val_sparse_categorical_accuracy: 0.0945\n",
            "Epoch 12/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 3.8940 - sparse_categorical_accuracy: 0.0977 - val_loss: 3.9642 - val_sparse_categorical_accuracy: 0.0964\n",
            "Epoch 13/32\n",
            "400/400 [==============================] - 18s 44ms/step - loss: 3.8431 - sparse_categorical_accuracy: 0.1074 - val_loss: 3.8328 - val_sparse_categorical_accuracy: 0.1087\n",
            "Epoch 14/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.7843 - sparse_categorical_accuracy: 0.1161 - val_loss: 3.9032 - val_sparse_categorical_accuracy: 0.1032\n",
            "Epoch 15/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.7518 - sparse_categorical_accuracy: 0.1205 - val_loss: 3.7937 - val_sparse_categorical_accuracy: 0.1173\n",
            "Epoch 16/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.7236 - sparse_categorical_accuracy: 0.1265 - val_loss: 3.7918 - val_sparse_categorical_accuracy: 0.1132\n",
            "Epoch 17/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.6928 - sparse_categorical_accuracy: 0.1320 - val_loss: 3.7406 - val_sparse_categorical_accuracy: 0.1304\n",
            "Epoch 18/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.6644 - sparse_categorical_accuracy: 0.1371 - val_loss: 3.7386 - val_sparse_categorical_accuracy: 0.1334\n",
            "Epoch 19/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 3.6421 - sparse_categorical_accuracy: 0.1416 - val_loss: 3.6995 - val_sparse_categorical_accuracy: 0.1344\n",
            "Epoch 20/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.6162 - sparse_categorical_accuracy: 0.1459 - val_loss: 3.7994 - val_sparse_categorical_accuracy: 0.1238\n",
            "Epoch 21/32\n",
            "400/400 [==============================] - 18s 44ms/step - loss: 3.5871 - sparse_categorical_accuracy: 0.1506 - val_loss: 3.6589 - val_sparse_categorical_accuracy: 0.1467\n",
            "Epoch 22/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.5593 - sparse_categorical_accuracy: 0.1580 - val_loss: 3.7115 - val_sparse_categorical_accuracy: 0.1365\n",
            "Epoch 23/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.5382 - sparse_categorical_accuracy: 0.1601 - val_loss: 3.6742 - val_sparse_categorical_accuracy: 0.1409\n",
            "Epoch 24/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.5259 - sparse_categorical_accuracy: 0.1619 - val_loss: 3.6632 - val_sparse_categorical_accuracy: 0.1424\n",
            "Epoch 25/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 3.5095 - sparse_categorical_accuracy: 0.1629 - val_loss: 3.6229 - val_sparse_categorical_accuracy: 0.1516\n",
            "Epoch 26/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 3.4732 - sparse_categorical_accuracy: 0.1744 - val_loss: 3.6249 - val_sparse_categorical_accuracy: 0.1613\n",
            "Epoch 27/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.4549 - sparse_categorical_accuracy: 0.1775 - val_loss: 3.6589 - val_sparse_categorical_accuracy: 0.1518\n",
            "Epoch 28/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.4341 - sparse_categorical_accuracy: 0.1790 - val_loss: 3.5773 - val_sparse_categorical_accuracy: 0.1641\n",
            "Epoch 29/32\n",
            "400/400 [==============================] - 18s 44ms/step - loss: 3.4202 - sparse_categorical_accuracy: 0.1808 - val_loss: 3.6237 - val_sparse_categorical_accuracy: 0.1544\n",
            "Epoch 30/32\n",
            "400/400 [==============================] - 17s 42ms/step - loss: 3.4014 - sparse_categorical_accuracy: 0.1867 - val_loss: 3.6035 - val_sparse_categorical_accuracy: 0.1606\n",
            "Epoch 31/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.3721 - sparse_categorical_accuracy: 0.1914 - val_loss: 3.5849 - val_sparse_categorical_accuracy: 0.1687\n",
            "Epoch 32/32\n",
            "400/400 [==============================] - 17s 43ms/step - loss: 3.3428 - sparse_categorical_accuracy: 0.1989 - val_loss: 3.5904 - val_sparse_categorical_accuracy: 0.1648\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 3.5872 - sparse_categorical_accuracy: 0.1646\n",
            "loss: 3.5871739387512207 , accuarcy: 0.16459999978542328 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized model"
      ],
      "metadata": {
        "id": "gmpNte01XY8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "batch_size = 128\n",
        "img_width, img_height, img_num_channels = 32, 32, 3\n",
        "loss_function = keras.losses.sparse_categorical_crossentropy\n",
        "no_classes = 100\n",
        "no_epochs = 10\n",
        "optimizer = keras.optimizers.Adam(0.001)\n",
        "validation_split = 0.2\n",
        "verbosity = 1\n",
        "\n",
        "# Load CIFAR-100 data\n",
        "(input_train, target_train), (input_test, target_test) = keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
        "\n",
        "# Determine shape of the data\n",
        "input_shape = (img_width, img_height, img_num_channels)\n",
        "\n",
        "# Parse numbers as floats\n",
        "input_train = input_train.astype('float32')\n",
        "input_test = input_test.astype('float32')\n",
        "\n",
        "# Normalize data\n",
        "input_train = input_train / 255\n",
        "input_test = input_test / 255\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same', strides=(2, 2)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding='same'))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding='same'))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=loss_function,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit data to model\n",
        "history = model.fit(input_train, target_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=no_epochs,\n",
        "            verbose=verbosity,\n",
        "            validation_split=validation_split)\n",
        "\n",
        "# Generate generalization metrics\n",
        "score = model.evaluate(input_test, target_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n"
      ],
      "metadata": {
        "id": "Lupg7VqoXjqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d206753a-df49-41e7-8702-2f374780b523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 97s 307ms/step - loss: 4.0232 - accuracy: 0.0868 - val_loss: 3.6348 - val_accuracy: 0.1448\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 97s 310ms/step - loss: 3.2892 - accuracy: 0.2054 - val_loss: 3.1157 - val_accuracy: 0.2431\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 96s 307ms/step - loss: 2.9311 - accuracy: 0.2704 - val_loss: 2.9740 - val_accuracy: 0.2747\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 95s 305ms/step - loss: 2.6893 - accuracy: 0.3177 - val_loss: 2.8696 - val_accuracy: 0.2945\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 97s 309ms/step - loss: 2.5066 - accuracy: 0.3580 - val_loss: 2.7298 - val_accuracy: 0.3220\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 96s 307ms/step - loss: 2.3481 - accuracy: 0.3913 - val_loss: 2.6887 - val_accuracy: 0.3344\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 95s 305ms/step - loss: 2.2052 - accuracy: 0.4189 - val_loss: 2.6689 - val_accuracy: 0.3366\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 98s 315ms/step - loss: 2.0715 - accuracy: 0.4497 - val_loss: 2.6534 - val_accuracy: 0.3458\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 99s 315ms/step - loss: 1.9421 - accuracy: 0.4808 - val_loss: 2.7070 - val_accuracy: 0.3512\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 98s 314ms/step - loss: 1.8260 - accuracy: 0.5049 - val_loss: 2.7886 - val_accuracy: 0.3392\n",
            "Test loss: 2.7349178791046143 / Test accuracy: 0.3506999909877777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transefe Learning"
      ],
      "metadata": {
        "id": "XWTbEfzZybpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Parse numbers as floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize data\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "\n",
        "#base pre-trained model\n",
        "base_model = VGG16(weights='imagenet',input_shape=(32, 32, 3), include_top=False)\n",
        "\n",
        "base_model.trainable = False #freeze the base model\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# and a logistic layer -- let's say we have 200 classes\n",
        "predictions = Dense(100, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs, outputs=predictions)\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.sparse_categorical_crossentropy,metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
        "\n",
        "# train the model on the new data for a few epochs\n",
        "model.fit(X_train, y_train,\n",
        "            batch_size=120,\n",
        "            epochs=2,\n",
        "            validation_split=.2)\n",
        "\n",
        "# chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# the first 249 layers and unfreeze the rest:\n",
        "for layer in base_model.layers[:250]:\n",
        "   layer.trainable = False\n",
        "for layer in base_model.layers[250:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "\n",
        "\n",
        "# recompile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=keras.losses.sparse_categorical_crossentropy,metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "            batch_size=120,\n",
        "            epochs=5,\n",
        "            verbose=1,\n",
        "            validation_split=.2)\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RQ9Ht73sTru",
        "outputId": "d35f58ad-6c3b-4f9a-f81c-24ba49d72817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "334/334 [==============================] - 535s 2s/step - loss: 3.7894 - sparse_categorical_accuracy: 0.1603 - val_loss: 3.3560 - val_sparse_categorical_accuracy: 0.2342\n",
            "Epoch 2/2\n",
            "334/334 [==============================] - 534s 2s/step - loss: 3.1641 - sparse_categorical_accuracy: 0.2593 - val_loss: 3.1057 - val_sparse_categorical_accuracy: 0.2677\n",
            "Epoch 1/5\n",
            "334/334 [==============================] - 528s 2s/step - loss: 3.0073 - sparse_categorical_accuracy: 0.2844 - val_loss: 3.0997 - val_sparse_categorical_accuracy: 0.2705\n",
            "Epoch 2/5\n",
            "334/334 [==============================] - 535s 2s/step - loss: 3.0020 - sparse_categorical_accuracy: 0.2873 - val_loss: 3.0954 - val_sparse_categorical_accuracy: 0.2720\n",
            "Epoch 3/5\n",
            "334/334 [==============================] - 551s 2s/step - loss: 2.9980 - sparse_categorical_accuracy: 0.2876 - val_loss: 3.0920 - val_sparse_categorical_accuracy: 0.2736\n",
            "Epoch 4/5\n",
            "334/334 [==============================] - 540s 2s/step - loss: 2.9947 - sparse_categorical_accuracy: 0.2894 - val_loss: 3.0892 - val_sparse_categorical_accuracy: 0.2736\n",
            "Epoch 5/5\n",
            "334/334 [==============================] - 526s 2s/step - loss: 2.9919 - sparse_categorical_accuracy: 0.2903 - val_loss: 3.0868 - val_sparse_categorical_accuracy: 0.2736\n",
            "Test loss: 3.0623857975006104 / Test accuracy: 0.27320000529289246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "_ApgbHRkrQxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Github](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-build-a-convnet-for-cifar-10-and-cifar-100-classification-with-keras.md#decomposition-of-images-into-smaller-generic-parts)\n",
        "\n",
        "2. [Kaggle](https://www.kaggle.com/code/clarillapanilla/cifar-100-cnn)\n",
        "\n",
        "3. [paperswithcode](https://paperswithcode.com/sota/class-incremental-learning-on-cifar100)\n",
        "\n",
        "4. [balajikulkarni](https://balajikulkarni.medium.com/transfer-learning-using-resnet-e20598314427)\n",
        "\n",
        "5. [keras model](https://keras.io/api/models/model/)"
      ],
      "metadata": {
        "id": "WR3i5tx-mLIG"
      }
    }
  ]
}
