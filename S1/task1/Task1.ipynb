{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imoprting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set (1077, 64) (1077,)\n",
      "val set (360, 64) (360,)\n",
      "test set (360, 64) (360,)\n"
     ]
    }
   ],
   "source": [
    "# before running make sure to put data inside data folder\n",
    "Train_data = loadmat(r\"data\\\\train.mat\")\n",
    "Val_data = loadmat(r\"data\\\\val.mat\")\n",
    "Test_data = loadmat(r\"data\\\\test.mat\")\n",
    "\n",
    "X_train = np.array(Train_data[\"features\"])\n",
    "Y_train = np.array(Train_data[\"label\"]).reshape(-1,)\n",
    "\n",
    "print(\"train set\",X_train.shape,Y_train.shape)\n",
    "\n",
    "X_val = np.array(Val_data[\"features\"])\n",
    "Y_val = np.array(Val_data[\"label\"]).reshape(-1,)\n",
    "\n",
    "print(\"val set\",X_val.shape,Y_val.shape)\n",
    "\n",
    "X_test = np.array(Test_data[\"features\"])\n",
    "Y_test = np.array(Test_data[\"label\"]).reshape(-1,)\n",
    "\n",
    "print(\"test set\",X_test.shape,Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Score 0.825\n",
      "test Score 0.8138888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "#build\n",
    "clf = tree.DecisionTreeClassifier(max_depth=12,random_state=43)\n",
    "\n",
    "# train \n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "#validate\n",
    "y_pred = clf.predict(X_val) # validate features\n",
    "print(\"val Score\" , accuracy_score(Y_val, y_pred))\n",
    "\n",
    "#test \n",
    "y_pred = clf.predict(X_test) # test features\n",
    "print(\"test Score\" , accuracy_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the max_depth was chosen ?\n",
    "i tried different numbers with high gaps to find underfitting and overfitting limits \n",
    "then i chose 12 as all higher numbers achieved same score (in validating and testing ) and all lower numbers acheived\n",
    "lower score\n",
    "\n",
    "#### for 95% accuracy Decision Tree will not work as Required  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Score 0.9861111111111112\n",
      "test Score 0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "#build\n",
    "neigh = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#train\n",
    "neigh = neigh.fit(X_train, Y_train)\n",
    "\n",
    "#validate\n",
    "y_pred = neigh.predict(X_val) # validate features\n",
    "print(\"val Score\" , accuracy_score(Y_val, y_pred))\n",
    "\n",
    "#test \n",
    "y_pred = neigh.predict(X_test) # test features\n",
    "print(\"test Score\" , accuracy_score(Y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the K was chosen ?\n",
    "after a little search I found that the most common methods to choose the value of k is: \n",
    "1. square root of dataset number \n",
    "2. if the dataset number is even, k should be odd and the opposite \n",
    "\n",
    "according to my tries , first method did not get the best accuracy so i tried odd numbers (as dataset is 1800) avoiding underfitting and overfitting until i chose k as 3\n",
    "\n",
    "#### for 95% accuracy K-NN will work as Required  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n",
      "0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#build\n",
    "#preprocessing to aviod errors of convergence\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression()) \n",
    "pipe.fit(X_train, Y_train)  \n",
    "\n",
    "#validate\n",
    "print(pipe.score(X_val, Y_val))\n",
    "\n",
    "#test\n",
    "print(pipe.score(X_test, Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for 95% accuracy Logistic Regression will work as Required as its not suitable for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e1fc3fa23fb62f3f701e1450a48be1e01c4f473533356521fe76f63c343b8fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
